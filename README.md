# Multilayer Perceptron (MLP) with NumPy
This repository contains an implementation of a Multilayer Perceptron (MLP) for both classification and regression tasks using only NumPy. The implementation includes various features such as batch normalization, activation functions, regularization, forward and backward propagation, optimization algorithms, and loss functions.

# Table of Contents
Introduction
Features
Installation
Usage
Examples
Contributing
License
Introduction
This project demonstrates how to build a Multilayer Perceptron from scratch using NumPy. It covers essential concepts in deep learning, including different optimization algorithms, activation functions, and regularization techniques.

# Features
Classification and Regression: Supports both types of tasks.
Batch Normalization: Accelerates training and improves performance.
Activation Functions: Includes ReLU, Sigmoid, Tanh, and more.
Regularization: Implements L2 regularization to prevent overfitting.
Forward and Backward Propagation: Efficiently computes gradients.
Optimization Algorithms: Supports Gradient Descent, Momentum, RMSprop, Adam, and Newtonâ€™s Method.
Loss Functions: Includes Mean Squared Error (MSE) and Cross-Entropy Loss.
# Installation
Clone the repository to your local machine:
```
git clone https://github.com/yourusername/mlp-numpy.git
cd mlp-numpy
```
